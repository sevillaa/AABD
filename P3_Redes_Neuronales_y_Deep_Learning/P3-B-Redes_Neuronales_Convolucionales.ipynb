{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 3.B - Redes Neuronales Convucionales\n",
    "Hecho por:\n",
    "- Jaime Benedí.\n",
    "- Miguel Sevilla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte vamos a trabajar con el dataset MNIST es un conjunto de datos en el que hay imágenes de números escritos a mano (del 0 al 9). Las dimensiones de las imágenes son de 28x28 píxeles y las imágenes están en escala de grises (1 canal). Hay diez posibles clases (los dígitos del 0 al 9). \n",
    "\n",
    "- Entrena y evalúa una red CNN con dos configuraciones distintas para el dataset MNIST, siguiendo el\n",
    "ejemplo visto en el tutorial. Las dos configuraciones tienen que tener:\n",
    "    - Un batch_size distinto.\n",
    "    - Distinto número de capas convolucionales.\n",
    "    - Distinto número de capas conectadas.\n",
    "    - Distinto tamaño de kernel.\n",
    "    - Distinto número de filtros (kernels).\n",
    "    - Distintos tamaños para reducir las matrices en la capa de pooling.\n",
    "    - Distintos números de neuronas en las capas ocultas de las capas conectadas.\n",
    "    - Una tasa de aprendizaje distinta.\n",
    "    - Distinto número de epochs.\n",
    "\n",
    "- Consejo: primero indica explícitamente (en markdown) estos hiperpárametros elegidos para las dos\n",
    "configuraciones. Después, haz el código. Explica el código en las partes donde hagas estas\n",
    "configuraciones.\n",
    "\n",
    "- Calcula el accuracy total y por clases para cada una de las configuraciones.\n",
    "- Puedes dibujar una gráfica de barras para ayudarte a comparar los resultados.\n",
    "- Explica las diferencias en las accuracies calculadas para ambas configuraciones y discute las razones de dichos resultados y posibles mejoras si es necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torch\n",
    "#import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils as tutils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#import matplotlib.pyplot as plt\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hiperparámetros:\n",
    "- batch_size = 40.\n",
    "- Número de capas convolucionales = 2\n",
    "- Número de capas conectadas = 2\n",
    "- Tamaño de kernel = 5\n",
    "- Número de filtros (kernels) = 3\n",
    "- Tamaño para reducir las matrices en la capa de pooling = 2\n",
    "- Número de neuronas en las capas ocultas de las capas conectadas (2592,100), (100, 10)\n",
    "- Tasa de aprendizaje = 0.1\n",
    "- Número de epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotsOfNeuronsNet_batch_size = 40\n",
    "lotsOfNeuronsNet_epochs = 20\n",
    "lotsOfNeuronsNet_learning_rate = 0.1\n",
    "\n",
    "class LotsOfNeuronsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LotsOfNeuronsNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 22, 5)\n",
    "        self.conv2 = nn.Conv2d(22, 32, 5)\n",
    "        self.pool1 = nn.MaxPool2d(3, 2)\n",
    "        self.fc1 = nn.Linear(2592, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hiperparámetros:\n",
    "- batch_size = 64.\n",
    "- Número de capas convolucionales = 2\n",
    "- Número de capas conectadas = 2\n",
    "- Tamaño de kernel. 5\n",
    "- Número de filtros (kernels) = 2\n",
    "- Tamaño para reducir las matrices en la capa de pooling = 2\n",
    "- Número de neuronas en las capas ocultas de las capas conectadas (512, 89), (89, 10)\n",
    "- Tasa de aprendizaje = 0.01\n",
    "- Número de epochs = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallNet_batch_size = 64\n",
    "smallNet_epochs = 14\n",
    "smallNet_learning_rate = 0.01\n",
    "\n",
    "class SmallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(512, 89)\n",
    "        self.fc2 = nn.Linear(89, 10)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de Torch y CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`device` será el medio hardware que se usará para la gestión de los cálculos de la red convolucional. El coste de cómputo es alto, y esta herramienta nos da una facilidad para ello usando la GPU del sistema, siempre y cuando disponga de una tecnología CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_SEED=0\n",
    "\n",
    "torch.manual_seed(TORCH_SEED)\n",
    "\n",
    "# Modificar estos booleanos para usar CUDA o MPS en función de preferencias personales\n",
    "doYouWantToUseCUDA = True\n",
    "doYouWantToUseMPS = True\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if doYouWantToUseCUDA and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif doYouWantToUseMPS and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "#else:\n",
    "    #device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estos objetos son necesarios para transformar los datos a arrays de PyTorch\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Cargamos el dataset MNIST. Tendremos la parte de entrenamiento y la de test por separado.\n",
    "# El dataset de entrenamiento tiene 60.000 imágenes y el de test 10.000 imágenes.\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Etiquetas de las clases de MNIST, que son los números del 0 al 9\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente objeto tendrá la función de cálculo del coste de la solución dada por el modelo. En nuestro caos, elegimos la función *entropy loss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construimos los modelos. Para ambas redes necesitamos:\n",
    "- La propia red, a la cual volcaremos sobre gpu si es posible.\n",
    "- Dos objetos para carga y manejo rápido de los datos de entrenamiento y prueba respectivamente, utilizados según las dinámicas de trabajo con PyTorch\n",
    "- El objeto \"optimizador\" de la red. Éste será el encargado de realizar el ajuste de los valores de la red para que en el entrenamiento realice una optimización de la clasificación\n",
    "    - Para ambos modelos utilizamos el `SGD` puesto que está pensado para imágenes, pero es necesario tener un objeto por modelo por la diferencia de parámetros de la red así como de la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotsOfNeuronsNet_model = LotsOfNeuronsNet().to(device)\n",
    "\n",
    "lotsOfNeuronsNetTrainLoader = tutils.data.DataLoader(trainset, batch_size=lotsOfNeuronsNet_batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "lotsOfNeuronsNetTestLoader = tutils.data.DataLoader(testset, batch_size=lotsOfNeuronsNet_batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "optimizerlotsOfNeuronsNet = optim.SGD(lotsOfNeuronsNet_model.parameters(), lr=lotsOfNeuronsNet_learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = SmallNet().to(device)\n",
    "\n",
    "smallTrainLoader = tutils.data.DataLoader(trainset, batch_size=smallNet_batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "smallTestLoader = tutils.data.DataLoader(testset, batch_size=smallNet_batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "optimizerSmallNet = optim.SGD(small_model.parameters(), lr=smallNet_learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el entrenamiento de cualquiera de ambas redes, seguimos la misma receta:\n",
    "1. Extraemos una submuestra (batch) con los datos y la salida esperada.\n",
    "2. Propagamos hacia adelante los datos por la red hasta recibir una salida.\n",
    "3. Se realiza un cálculo del error cometido con esa salida respecto a la salida esperada.\n",
    "4. Propagamos hacia atrás ese error por la red para ver como de alejadas están las aproximaciones en las capas, neuronas, etc. obteniendo los gradientes de los cálculos.\n",
    "5. Optimizamos con esos gradientes más el algoritmo de optimización los valores de la red.\n",
    "6. Repetimos ese proceso por cada batch y por cada epoch (reiniciando los valores del optimizador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, \n",
    "    trainloader, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    epochs, \n",
    "    device=device, \n",
    "    verbose=True  \n",
    ") :\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if verbose and batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epochs} [{batch_idx * len(data)}/{len(trainloader.dataset)} ({100. * batch_idx / len(trainloader):.0f}%)]\\tLoss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo con muchas neuronas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.337664\n",
      "Train Epoch: 0 [4000/60000 (7%)]\tLoss: 1.047639\n",
      "Train Epoch: 0 [8000/60000 (13%)]\tLoss: 0.716011\n",
      "Train Epoch: 0 [12000/60000 (20%)]\tLoss: 0.992388\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.643911\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.471602\n",
      "Train Epoch: 0 [24000/60000 (40%)]\tLoss: 0.736041\n",
      "Train Epoch: 0 [28000/60000 (47%)]\tLoss: 0.629040\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.399078\n",
      "Train Epoch: 0 [36000/60000 (60%)]\tLoss: 0.369858\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.393410\n",
      "Train Epoch: 0 [44000/60000 (73%)]\tLoss: 0.178129\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.605569\n",
      "Train Epoch: 0 [52000/60000 (87%)]\tLoss: 0.677212\n",
      "Train Epoch: 0 [56000/60000 (93%)]\tLoss: 0.529048\n",
      "Epoch 0 finished\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.567550\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.432897\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.152767\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.243168\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.222830\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.145229\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.395967\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.562039\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.293505\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.677781\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.223661\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.277584\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.326325\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.304739\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.377322\n",
      "Epoch 1 finished\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.187808\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.482764\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.704125\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.555686\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.336688\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.358751\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.200220\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.627536\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.476760\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.505408\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.393590\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.329149\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.140581\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.483758\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.238678\n",
      "Epoch 2 finished\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.147308\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.261746\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.190381\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.176824\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.382931\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.344715\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.323572\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.167297\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.495397\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.332068\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.168790\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.173240\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.314147\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.423696\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.321655\n",
      "Epoch 3 finished\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.080590\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.072382\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.153095\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.196450\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.365190\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.227716\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.037302\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.260992\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.352326\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.304168\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.066775\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 1.076525\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.127230\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.391319\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.285210\n",
      "Epoch 4 finished\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.174291\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.251478\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.687646\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.385452\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.181073\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.467279\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.102625\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.680083\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.093647\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.139695\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.496990\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.353098\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.492991\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.124604\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.043171\n",
      "Epoch 5 finished\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.034282\n",
      "Train Epoch: 6 [4000/60000 (7%)]\tLoss: 0.225495\n",
      "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 0.446979\n",
      "Train Epoch: 6 [12000/60000 (20%)]\tLoss: 0.701873\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.336746\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.252423\n",
      "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 0.448794\n",
      "Train Epoch: 6 [28000/60000 (47%)]\tLoss: 0.098445\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.554149\n",
      "Train Epoch: 6 [36000/60000 (60%)]\tLoss: 0.148988\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.253949\n",
      "Train Epoch: 6 [44000/60000 (73%)]\tLoss: 0.525308\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.130775\n",
      "Train Epoch: 6 [52000/60000 (87%)]\tLoss: 0.523782\n",
      "Train Epoch: 6 [56000/60000 (93%)]\tLoss: 0.564770\n",
      "Epoch 6 finished\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.228119\n",
      "Train Epoch: 7 [4000/60000 (7%)]\tLoss: 0.308193\n",
      "Train Epoch: 7 [8000/60000 (13%)]\tLoss: 0.210633\n",
      "Train Epoch: 7 [12000/60000 (20%)]\tLoss: 0.284898\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.512308\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.044527\n",
      "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 0.086255\n",
      "Train Epoch: 7 [28000/60000 (47%)]\tLoss: 0.245530\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.228496\n",
      "Train Epoch: 7 [36000/60000 (60%)]\tLoss: 0.127796\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.408338\n",
      "Train Epoch: 7 [44000/60000 (73%)]\tLoss: 0.406292\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.385620\n",
      "Train Epoch: 7 [52000/60000 (87%)]\tLoss: 0.581136\n",
      "Train Epoch: 7 [56000/60000 (93%)]\tLoss: 0.064207\n",
      "Epoch 7 finished\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.109998\n",
      "Train Epoch: 8 [4000/60000 (7%)]\tLoss: 0.067713\n",
      "Train Epoch: 8 [8000/60000 (13%)]\tLoss: 0.454998\n",
      "Train Epoch: 8 [12000/60000 (20%)]\tLoss: 0.183318\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.329930\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.583755\n",
      "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 0.104485\n",
      "Train Epoch: 8 [28000/60000 (47%)]\tLoss: 0.346643\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.850502\n",
      "Train Epoch: 8 [36000/60000 (60%)]\tLoss: 0.357726\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.183357\n",
      "Train Epoch: 8 [44000/60000 (73%)]\tLoss: 0.361538\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.383052\n",
      "Train Epoch: 8 [52000/60000 (87%)]\tLoss: 0.157849\n",
      "Train Epoch: 8 [56000/60000 (93%)]\tLoss: 0.090935\n",
      "Epoch 8 finished\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.212208\n",
      "Train Epoch: 9 [4000/60000 (7%)]\tLoss: 0.787818\n",
      "Train Epoch: 9 [8000/60000 (13%)]\tLoss: 0.133412\n",
      "Train Epoch: 9 [12000/60000 (20%)]\tLoss: 0.340053\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.172917\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.142277\n",
      "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 0.334177\n",
      "Train Epoch: 9 [28000/60000 (47%)]\tLoss: 0.172587\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.088677\n",
      "Train Epoch: 9 [36000/60000 (60%)]\tLoss: 0.319844\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.181718\n",
      "Train Epoch: 9 [44000/60000 (73%)]\tLoss: 0.317411\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.197734\n",
      "Train Epoch: 9 [52000/60000 (87%)]\tLoss: 0.319551\n",
      "Train Epoch: 9 [56000/60000 (93%)]\tLoss: 0.302911\n",
      "Epoch 9 finished\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.221693\n",
      "Train Epoch: 10 [4000/60000 (7%)]\tLoss: 0.490310\n",
      "Train Epoch: 10 [8000/60000 (13%)]\tLoss: 0.469427\n",
      "Train Epoch: 10 [12000/60000 (20%)]\tLoss: 0.046010\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.374962\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.340747\n",
      "Train Epoch: 10 [24000/60000 (40%)]\tLoss: 0.241731\n",
      "Train Epoch: 10 [28000/60000 (47%)]\tLoss: 0.691413\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.191336\n",
      "Train Epoch: 10 [36000/60000 (60%)]\tLoss: 0.535764\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.358859\n",
      "Train Epoch: 10 [44000/60000 (73%)]\tLoss: 0.373192\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.067340\n",
      "Train Epoch: 10 [52000/60000 (87%)]\tLoss: 0.004157\n",
      "Train Epoch: 10 [56000/60000 (93%)]\tLoss: 0.281670\n",
      "Epoch 10 finished\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.261993\n",
      "Train Epoch: 11 [4000/60000 (7%)]\tLoss: 0.285565\n",
      "Train Epoch: 11 [8000/60000 (13%)]\tLoss: 0.202376\n",
      "Train Epoch: 11 [12000/60000 (20%)]\tLoss: 0.199902\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.117940\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 0.042080\n",
      "Train Epoch: 11 [24000/60000 (40%)]\tLoss: 0.352477\n",
      "Train Epoch: 11 [28000/60000 (47%)]\tLoss: 0.096679\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.214251\n",
      "Train Epoch: 11 [36000/60000 (60%)]\tLoss: 0.112514\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 0.173759\n",
      "Train Epoch: 11 [44000/60000 (73%)]\tLoss: 0.339761\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.344374\n",
      "Train Epoch: 11 [52000/60000 (87%)]\tLoss: 0.340348\n",
      "Train Epoch: 11 [56000/60000 (93%)]\tLoss: 0.404828\n",
      "Epoch 11 finished\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.216730\n",
      "Train Epoch: 12 [4000/60000 (7%)]\tLoss: 0.252929\n",
      "Train Epoch: 12 [8000/60000 (13%)]\tLoss: 0.589975\n",
      "Train Epoch: 12 [12000/60000 (20%)]\tLoss: 0.134887\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.135234\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 0.189996\n",
      "Train Epoch: 12 [24000/60000 (40%)]\tLoss: 0.529066\n",
      "Train Epoch: 12 [28000/60000 (47%)]\tLoss: 0.273935\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.217889\n",
      "Train Epoch: 12 [36000/60000 (60%)]\tLoss: 0.218175\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 0.387359\n",
      "Train Epoch: 12 [44000/60000 (73%)]\tLoss: 0.529126\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.016280\n",
      "Train Epoch: 12 [52000/60000 (87%)]\tLoss: 0.343054\n",
      "Train Epoch: 12 [56000/60000 (93%)]\tLoss: 0.433927\n",
      "Epoch 12 finished\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.083895\n",
      "Train Epoch: 13 [4000/60000 (7%)]\tLoss: 0.202286\n",
      "Train Epoch: 13 [8000/60000 (13%)]\tLoss: 0.065854\n",
      "Train Epoch: 13 [12000/60000 (20%)]\tLoss: 0.034968\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.153454\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 0.390376\n",
      "Train Epoch: 13 [24000/60000 (40%)]\tLoss: 0.646721\n",
      "Train Epoch: 13 [28000/60000 (47%)]\tLoss: 0.165543\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.296571\n",
      "Train Epoch: 13 [36000/60000 (60%)]\tLoss: 0.267113\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 0.063549\n",
      "Train Epoch: 13 [44000/60000 (73%)]\tLoss: 0.263770\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.314490\n",
      "Train Epoch: 13 [52000/60000 (87%)]\tLoss: 0.389332\n",
      "Train Epoch: 13 [56000/60000 (93%)]\tLoss: 0.177434\n",
      "Epoch 13 finished\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.363200\n",
      "Train Epoch: 14 [4000/60000 (7%)]\tLoss: 0.640753\n",
      "Train Epoch: 14 [8000/60000 (13%)]\tLoss: 0.121631\n",
      "Train Epoch: 14 [12000/60000 (20%)]\tLoss: 0.128259\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.431101\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 0.508324\n",
      "Train Epoch: 14 [24000/60000 (40%)]\tLoss: 0.245380\n",
      "Train Epoch: 14 [28000/60000 (47%)]\tLoss: 0.461126\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.418386\n",
      "Train Epoch: 14 [36000/60000 (60%)]\tLoss: 0.344281\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 0.196457\n",
      "Train Epoch: 14 [44000/60000 (73%)]\tLoss: 0.082184\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.265370\n",
      "Train Epoch: 14 [52000/60000 (87%)]\tLoss: 0.189075\n",
      "Train Epoch: 14 [56000/60000 (93%)]\tLoss: 0.300450\n",
      "Epoch 14 finished\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.273913\n",
      "Train Epoch: 15 [4000/60000 (7%)]\tLoss: 0.413155\n",
      "Train Epoch: 15 [8000/60000 (13%)]\tLoss: 0.178284\n",
      "Train Epoch: 15 [12000/60000 (20%)]\tLoss: 0.624769\n",
      "Train Epoch: 15 [16000/60000 (27%)]\tLoss: 0.224522\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 0.228937\n",
      "Train Epoch: 15 [24000/60000 (40%)]\tLoss: 0.477336\n",
      "Train Epoch: 15 [28000/60000 (47%)]\tLoss: 0.368260\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.484508\n",
      "Train Epoch: 15 [36000/60000 (60%)]\tLoss: 0.156265\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 0.524070\n",
      "Train Epoch: 15 [44000/60000 (73%)]\tLoss: 0.609451\n",
      "Train Epoch: 15 [48000/60000 (80%)]\tLoss: 0.323292\n",
      "Train Epoch: 15 [52000/60000 (87%)]\tLoss: 0.127060\n",
      "Train Epoch: 15 [56000/60000 (93%)]\tLoss: 0.130271\n",
      "Epoch 15 finished\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.810035\n",
      "Train Epoch: 16 [4000/60000 (7%)]\tLoss: 0.623215\n",
      "Train Epoch: 16 [8000/60000 (13%)]\tLoss: 0.205756\n",
      "Train Epoch: 16 [12000/60000 (20%)]\tLoss: 0.194703\n",
      "Train Epoch: 16 [16000/60000 (27%)]\tLoss: 0.271881\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 0.220997\n",
      "Train Epoch: 16 [24000/60000 (40%)]\tLoss: 0.209524\n",
      "Train Epoch: 16 [28000/60000 (47%)]\tLoss: 0.397093\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.180616\n",
      "Train Epoch: 16 [36000/60000 (60%)]\tLoss: 0.328180\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 0.299475\n",
      "Train Epoch: 16 [44000/60000 (73%)]\tLoss: 0.469544\n",
      "Train Epoch: 16 [48000/60000 (80%)]\tLoss: 0.581793\n",
      "Train Epoch: 16 [52000/60000 (87%)]\tLoss: 0.216592\n",
      "Train Epoch: 16 [56000/60000 (93%)]\tLoss: 0.556621\n",
      "Epoch 16 finished\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.606750\n",
      "Train Epoch: 17 [4000/60000 (7%)]\tLoss: 0.544670\n",
      "Train Epoch: 17 [8000/60000 (13%)]\tLoss: 0.285660\n",
      "Train Epoch: 17 [12000/60000 (20%)]\tLoss: 0.106244\n",
      "Train Epoch: 17 [16000/60000 (27%)]\tLoss: 0.261597\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 0.131774\n",
      "Train Epoch: 17 [24000/60000 (40%)]\tLoss: 0.394276\n",
      "Train Epoch: 17 [28000/60000 (47%)]\tLoss: 0.319318\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.486162\n",
      "Train Epoch: 17 [36000/60000 (60%)]\tLoss: 0.355171\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 0.411127\n",
      "Train Epoch: 17 [44000/60000 (73%)]\tLoss: 0.196963\n",
      "Train Epoch: 17 [48000/60000 (80%)]\tLoss: 0.345067\n",
      "Train Epoch: 17 [52000/60000 (87%)]\tLoss: 0.359407\n",
      "Train Epoch: 17 [56000/60000 (93%)]\tLoss: 0.489822\n",
      "Epoch 17 finished\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.112802\n",
      "Train Epoch: 18 [4000/60000 (7%)]\tLoss: 0.512961\n",
      "Train Epoch: 18 [8000/60000 (13%)]\tLoss: 0.677513\n",
      "Train Epoch: 18 [12000/60000 (20%)]\tLoss: 0.249057\n",
      "Train Epoch: 18 [16000/60000 (27%)]\tLoss: 0.266994\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 0.252823\n",
      "Train Epoch: 18 [24000/60000 (40%)]\tLoss: 0.396521\n",
      "Train Epoch: 18 [28000/60000 (47%)]\tLoss: 0.436819\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.133611\n",
      "Train Epoch: 18 [36000/60000 (60%)]\tLoss: 0.376329\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 0.393230\n",
      "Train Epoch: 18 [44000/60000 (73%)]\tLoss: 0.113584\n",
      "Train Epoch: 18 [48000/60000 (80%)]\tLoss: 0.357042\n",
      "Train Epoch: 18 [52000/60000 (87%)]\tLoss: 0.182720\n",
      "Train Epoch: 18 [56000/60000 (93%)]\tLoss: 0.207275\n",
      "Epoch 18 finished\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.130219\n",
      "Train Epoch: 19 [4000/60000 (7%)]\tLoss: 0.316566\n",
      "Train Epoch: 19 [8000/60000 (13%)]\tLoss: 0.318448\n",
      "Train Epoch: 19 [12000/60000 (20%)]\tLoss: 0.329549\n",
      "Train Epoch: 19 [16000/60000 (27%)]\tLoss: 0.188394\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 0.334248\n",
      "Train Epoch: 19 [24000/60000 (40%)]\tLoss: 0.179755\n",
      "Train Epoch: 19 [28000/60000 (47%)]\tLoss: 0.209160\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.586252\n",
      "Train Epoch: 19 [36000/60000 (60%)]\tLoss: 0.401379\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 0.173594\n",
      "Train Epoch: 19 [44000/60000 (73%)]\tLoss: 0.270826\n",
      "Train Epoch: 19 [48000/60000 (80%)]\tLoss: 0.235505\n",
      "Train Epoch: 19 [52000/60000 (87%)]\tLoss: 0.138761\n",
      "Train Epoch: 19 [56000/60000 (93%)]\tLoss: 0.476959\n",
      "Epoch 19 finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(lotsOfNeuronsNet_epochs):\n",
    "    train(lotsOfNeuronsNet_model, lotsOfNeuronsNetTrainLoader, optimizerlotsOfNeuronsNet, criterion, epoch, device)\n",
    "    print(f\"Epoch {epoch} finished\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento de la red pequeña:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.309393\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.209870\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.290261\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.237933\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.059849\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.034393\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.067096\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.205432\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.011079\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.035760\n",
      "Epoch 0 finished\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.125689\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.083335\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.055901\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.050035\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.096188\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.014877\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.149201\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.005410\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.017626\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.140671\n",
      "Epoch 1 finished\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.018479\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.026426\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.039669\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.010961\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.008012\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.014996\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.077251\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.088021\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.042062\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.047577\n",
      "Epoch 2 finished\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.019793\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.006758\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.011130\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.038917\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.001595\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.001147\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.002574\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.006975\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.029658\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.000772\n",
      "Epoch 3 finished\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.036524\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.002392\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.016731\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.014091\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.010544\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.010704\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.008354\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.004506\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.008089\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.002570\n",
      "Epoch 4 finished\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.002837\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.002419\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.009697\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.006994\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.020571\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.000784\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.005021\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.027263\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.000963\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.025567\n",
      "Epoch 5 finished\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.002788\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.014701\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.003275\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.015441\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.000979\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.015847\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.000260\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.000491\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.011974\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.038117\n",
      "Epoch 6 finished\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.005282\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.030448\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.006480\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.000755\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.001820\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001266\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.000655\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.008014\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.002026\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.007115\n",
      "Epoch 7 finished\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.009896\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.000252\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.000668\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.032086\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.001174\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.003107\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.007977\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.019928\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.001264\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.000438\n",
      "Epoch 8 finished\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.005661\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.000549\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.001695\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000631\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.081161\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.032791\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.014623\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002368\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.003262\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000457\n",
      "Epoch 9 finished\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.000279\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.000485\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000863\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.006138\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.000974\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.000091\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.070109\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.003254\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.001254\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.000132\n",
      "Epoch 10 finished\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.001119\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.005288\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.000805\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.000618\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.000333\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.000860\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.000177\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.000015\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.000346\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.000221\n",
      "Epoch 11 finished\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.000047\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.003088\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.036385\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.000342\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.015214\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.000083\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.015488\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.000314\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.001033\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.002762\n",
      "Epoch 12 finished\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.001936\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.000168\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.000143\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.001223\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.001436\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.000168\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.001296\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.000159\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.000118\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.008079\n",
      "Epoch 13 finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(smallNet_epochs):\n",
    "    train(small_model, smallTrainLoader, optimizerSmallNet, criterion, epoch, device)\n",
    "    print(f\"Epoch {epoch} finished\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta función, ayudándonos de la función de coste, recabamos la pérdida promedio así como contar el número de aciertos del modelo sobre los datosde prueba señalados por el `testLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    model, \n",
    "    testloader, \n",
    "    criterion, \n",
    "    device=device, \n",
    "    verbose=True  \n",
    ") :\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            _, pred = torch.max(output, 1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'\\nTest set: Average loss: {test_loss / len(testloader.dataset):.4f}, Accuracy: {correct}/{len(testloader.dataset)} ({100. * correct / len(testloader.dataset):.0f}%)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta función, generamos predicciones para los datos y observamos el acierto cometido para cada dato respecto a la pertenencia de cada clase, lo que nos da la precisión por clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_per_class(\n",
    "    model,\n",
    "    testloader,\n",
    "    classes,\n",
    "    device=device,\n",
    ") :\n",
    "    # diccionarios para contar las predicciones correctas y el total de predicciones por clase, inicializados a 0\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            \n",
    "            # contamos las predicciones correctas y el total de predicciones por clase\n",
    "            for label, prediction in zip(target, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "    # verbose per class\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0067, Accuracy: 9193/10000 (92%)\n",
      "\n",
      "Accuracy for class: 0     is 94.6 %\n",
      "Accuracy for class: 1     is 97.6 %\n",
      "Accuracy for class: 2     is 92.1 %\n",
      "Accuracy for class: 3     is 89.9 %\n",
      "Accuracy for class: 4     is 90.9 %\n",
      "Accuracy for class: 5     is 87.1 %\n",
      "Accuracy for class: 6     is 96.1 %\n",
      "Accuracy for class: 7     is 88.6 %\n",
      "Accuracy for class: 8     is 93.8 %\n",
      "Accuracy for class: 9     is 87.6 %\n"
     ]
    }
   ],
   "source": [
    "test(lotsOfNeuronsNet_model, lotsOfNeuronsNetTestLoader, criterion, device)\n",
    "test_per_class(lotsOfNeuronsNet_model, lotsOfNeuronsNetTestLoader, classes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 9924/10000 (99%)\n",
      "\n",
      "Accuracy for class: 0     is 99.8 %\n",
      "Accuracy for class: 1     is 99.8 %\n",
      "Accuracy for class: 2     is 99.3 %\n",
      "Accuracy for class: 3     is 99.1 %\n",
      "Accuracy for class: 4     is 98.7 %\n",
      "Accuracy for class: 5     is 99.0 %\n",
      "Accuracy for class: 6     is 99.1 %\n",
      "Accuracy for class: 7     is 99.1 %\n",
      "Accuracy for class: 8     is 99.6 %\n",
      "Accuracy for class: 9     is 98.8 %\n"
     ]
    }
   ],
   "source": [
    "test(small_model, smallTestLoader, criterion, device)\n",
    "test_per_class(small_model, smallTestLoader, classes, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos entre los dos modelos como la red pequeña obtiene mejores resultados, tanto generales como por clase. Esto se puede deber a que para ambas capas convolucionales tienen su capa de pooling, lo que garantiza una mejor eliminación de ruido, así como un redimensionado más adecuado a las matrices usadas. El uso de más capas es compensado con un menor uso significativo de neuronas, lo cual afecta al coste en tiempo para bien. Su tasa de aprendizaje es baja por neurona, lo que implica que la opción correcta de evaluación del aprendizaje por cómputo es pequeña. El batch size es mayor, lo cual implica que compensa el proceso de aprendizaje por muchas secciones del dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
